{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Tutorial 03: Define Networks\n",
    "## Overview\n",
    "In this tutorial, we explain how to define networks. \n",
    "在pytorch中定义网络的方式：①sequential模块②复杂网络一般用nn模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.8981, 0.3694, 0.0370,  ..., 0.9766, 0.0271, 0.8501],\n",
       "         [0.5021, 0.7461, 0.1581,  ..., 0.7167, 0.1369, 0.2732],\n",
       "         [0.4663, 0.6417, 0.3524,  ..., 0.9466, 0.0888, 0.0839],\n",
       "         ...,\n",
       "         [0.9165, 0.6616, 0.6528,  ..., 0.1103, 0.6601, 0.4323],\n",
       "         [0.1486, 0.8005, 0.7195,  ..., 0.7407, 0.0810, 0.5490],\n",
       "         [0.8683, 0.5279, 0.6342,  ..., 0.4721, 0.1308, 0.0269]],\n",
       "\n",
       "        [[0.8770, 0.4527, 0.1853,  ..., 0.9056, 0.5798, 0.9272],\n",
       "         [0.5516, 0.6729, 0.4450,  ..., 0.9623, 0.0930, 0.3770],\n",
       "         [0.5665, 0.0622, 0.8109,  ..., 0.1841, 0.5201, 0.7251],\n",
       "         ...,\n",
       "         [0.0133, 0.1244, 0.1372,  ..., 0.1211, 0.2357, 0.2388],\n",
       "         [0.5162, 0.1379, 0.0815,  ..., 0.9151, 0.2089, 0.7587],\n",
       "         [0.9097, 0.1936, 0.3139,  ..., 0.8380, 0.8295, 0.0303]],\n",
       "\n",
       "        [[0.2070, 0.0439, 0.5768,  ..., 0.6182, 0.7775, 0.6866],\n",
       "         [0.1776, 0.8487, 0.7369,  ..., 0.7675, 0.0982, 0.5767],\n",
       "         [0.1524, 0.8180, 0.0434,  ..., 0.6283, 0.2910, 0.1080],\n",
       "         ...,\n",
       "         [0.7218, 0.3480, 0.0323,  ..., 0.3823, 0.9253, 0.8923],\n",
       "         [0.4865, 0.3855, 0.2021,  ..., 0.7699, 0.1183, 0.4884],\n",
       "         [0.0754, 0.9171, 0.6043,  ..., 0.0989, 0.6175, 0.3250]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.3992, 0.7690, 0.4592,  ..., 0.7215, 0.8321, 0.2232],\n",
       "         [0.4614, 0.8827, 0.7115,  ..., 0.7684, 0.9882, 0.5115],\n",
       "         [0.2516, 0.7719, 0.3845,  ..., 0.0565, 0.9355, 0.8566],\n",
       "         ...,\n",
       "         [0.7963, 0.0495, 0.2897,  ..., 0.9479, 0.7815, 0.1393],\n",
       "         [0.6778, 0.1299, 0.1306,  ..., 0.3842, 0.8796, 0.5013],\n",
       "         [0.3029, 0.6620, 0.7329,  ..., 0.1082, 0.2451, 0.4815]],\n",
       "\n",
       "        [[0.1148, 0.4765, 0.9783,  ..., 0.2384, 0.5215, 0.5168],\n",
       "         [0.4384, 0.4490, 0.1522,  ..., 0.9115, 0.4181, 0.3492],\n",
       "         [0.7940, 0.9772, 0.8695,  ..., 0.8390, 0.4729, 0.2651],\n",
       "         ...,\n",
       "         [0.6848, 0.0915, 0.6047,  ..., 0.6422, 0.7923, 0.7228],\n",
       "         [0.0868, 0.8009, 0.8880,  ..., 0.0397, 0.0600, 0.0048],\n",
       "         [0.2652, 0.3596, 0.2518,  ..., 0.9030, 0.7822, 0.0872]],\n",
       "\n",
       "        [[0.0274, 0.2239, 0.9131,  ..., 0.1756, 0.0755, 0.8956],\n",
       "         [0.6425, 0.1741, 0.6132,  ..., 0.7547, 0.6019, 0.5804],\n",
       "         [0.4754, 0.5526, 0.7285,  ..., 0.6187, 0.5396, 0.2075],\n",
       "         ...,\n",
       "         [0.9098, 0.9320, 0.2836,  ..., 0.0813, 0.4237, 0.6285],\n",
       "         [0.5084, 0.2729, 0.4423,  ..., 0.7540, 0.1960, 0.5183],\n",
       "         [0.0465, 0.8318, 0.2907,  ..., 0.7639, 0.9889, 0.7560]]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand([8, 100, 10]).detach()#detach()?\n",
    "x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 1, 1, 1, 0, 0, 1], dtype=torch.int32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.rand(8)\n",
    "y = (y>0.5).int()#大于0.5的变1，小于0.5的变0\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#建议每一行运行一下，了解每个量的输入和输出是什么\n",
    "class MLP(nn.Module):#继承nn.Module;MLP:多层感知网络（全连接层）\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.first_layer = nn.Linear(1000,50)#输入1000维，输出50维\n",
    "        self.second_layer = nn.Linear(50, 1)\n",
    "        #为什么把网络写在_init_()中？init中注册了之后，才知道哪些参数进行优化\n",
    "        #把多个线性层反复直接堆起来？（如多个first_layer堆起来）权重没有变化\n",
    "        #最终的维度与y有关，二分类问题，最后转换成一维概率即可;如果是多分类问题，比如3分类，那么最终的layer的输出实际上应该是3\n",
    "    def forward(self, x):#x：每个batch的输入\n",
    "        x = torch.flatten(x, start_dim=1, end_dim=2)#把x的后面两个维度拉直，变成了1000维的向量\n",
    "        x = nn.functional.relu(self.first_layer(x))#并没有把relu做init，因为至少标准的relu中，没有需要求解的参数或权重\n",
    "        #假设relu中还有一些参数需要我们求解，则需要先进行初始化，否则无初始化且不会进行恰当优化\n",
    "        x = self.second_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLP()#实例化这个网络能不能也很简单？\n",
    "output = mlp(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1922],\n",
       "        [-0.1654],\n",
       "        [-0.2327],\n",
       "        [-0.2706],\n",
       "        [-0.2534],\n",
       "        [-0.2508],\n",
       "        [-0.2221],\n",
       "        [-0.2470]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output#output基本随机：没有强调参数，也没有刻意初始化——默认的初始化；因为是线性层，所以量纲理论上是正负无穷之间，这里因为初始化原因，都接近0\n",
    "#如果要转换为概率，可以做一个Logit或sofrmax的操作，这里并没有做，会最终在loss函数中去做"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):#一个把对应的如012345678以及到输入有多少个这样的词汇或者多少个Entity，应用成多少维\n",
    "    def __init__(self):\n",
    "        super(Embedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(4, 100)#输入词汇串中最大的数值（字母？）：只能由数字0、1、2、3构成，不能是4、5、6...，每个词汇对应为100维向量(embbeding的dim)\n",
    "    def forward(self, x):\n",
    "        return self.embedding(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = Embedding()\n",
    "\n",
    "embedding_input = torch.tensor([[0,1, 0],[2,3, 3]])\n",
    "embedding_output = embedding(embedding_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 100])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(10, \n",
    "                           15, \n",
    "                           num_layers=2, \n",
    "                           bidirectional=True, \n",
    "                           dropout=0.1)#10：Embbeding的dim；15：隐藏层的维度（hidden dim）；\n",
    "        #layer：一共把这LSTM的多少层（多层叠加：下面LSTM的隐藏层的输出等于上面一层LSTM的输入，对传统词向量该层数可能是2，最多是3，如果是BERT作为LSTM的输入的话，多层也无实际帮助）\n",
    "        #bidirectional：在进行文本操作时，需要对其方向做一个指定，因为有时候语义不仅取决于我之前说了什么，也取决于之后说的什么，当该值=true时，我既要将该向量按照前向后输入一遍，接着还要从后向前输一遍\n",
    "        #dropout：最后一层的dropout\n",
    "    def forward(self, x):\n",
    "        output, (hidden, cell) = self.lstm(x)\n",
    "        return output, hidden, cell#三个输出：output是它每一个位置的hidden，最终一层它每一个timestep的输出，而hidden只是最终状态的输出，cell是hidden中的一些状态\n",
    "        #我们最终有4个hidden：num_layers=2,bidirectional=True\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "permute_x = x.permute([1,0,2])\n",
    " #MLP中，我们假设第一维是观测数，而在LSTM中实际上给它观测的是第二位，因此要对MLP做一个permute操作才能和LSTM保持一致：把它的第二维移成第一维，第三维保持不动\n",
    "lstm = LSTM()\n",
    "output_lstm1, output_lstm2, output_lstm3 = lstm(permute_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 8, 30])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_lstm1.shape#output_lstm1第一维是序列长度，第二维才是观测数batch size——permute,30=15*2，往前和往后拿15\n",
    "#output_lstm2第二维是batch size没有变，因为是最终的输出，实际返回不止最后一层，还包括之前层，第一个维度是4（4个hidden）, 15=hidden dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv(nn.Module):#convolution\n",
    "    def __init__(self):\n",
    "        super(Conv, self).__init__()\n",
    "        self.conv1d = nn.Conv1d(100, 50, 2)#1d层数，100：in_channel；50：out_channel；2：kernel；卷积是二维的，所以kernel_size=2\n",
    "    def forward(self, x):\n",
    "        return self.conv1d(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = Conv()\n",
    "output = conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 50, 9])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape#channel的数量从100变成50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最复杂的问题：维度——把其中的一些层拿出来进行测试"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
